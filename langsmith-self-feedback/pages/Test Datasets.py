from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate
from langchain_openai import ChatOpenAI
from langsmith import Client
import streamlit as st
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.messages import ChatMessage

st.set_page_config(page_title="Self Learning GPT í…ŒìŠ¤íŠ¸", page_icon="ğŸ¦œ")
st.title("ğŸ¦œ Self Learning GPT í…ŒìŠ¤íŠ¸")

# API KEY ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤
from dotenv import load_dotenv, find_dotenv

load_dotenv(find_dotenv())

# LangSmith ì„¤ì •
client = Client()

class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text)


def create_qa_pairs(dataset_id, session_id=None):
    qa_pairs = []
    examples = client.list_examples(dataset_id=dataset_id)
    for exp in examples:
        qa = dict()
        ret = extract_qa_from_runid(exp.source_run_id, session_id)

        if ret is None:
            continue
        qa["question"] = ret["question"]
        qa["answer"] = ret["answer"]
        qa["session_id"] = ret["session_id"]
        qa_pairs.append(qa)
    return qa_pairs


def extract_qa_from_runid(run_id, session_id=None):
    ret = dict()
    filter_flag = True
    for k, v in client.read_run(run_id):
        if k == "outputs":
            ret["answer"] = v["output"]["content"]
        if k == "inputs":
            ret["question"] = v["input"]
        if k == "session_id":
            ret["session_id"] = v
    if filter_flag:
        return ret
    else:
        return None

reset_history = st.sidebar.button("ëŒ€í™”ë‚´ìš© ì´ˆê¸°í™”", type="primary")
if reset_history:
    st.session_state["last_run"] = None
    st.session_state.messages = []

if "messages" not in st.session_state:
    st.session_state["messages"] = []

for msg in st.session_state.messages:
    st.chat_message(msg.role).write(msg.content)

with st.sidebar:
    dataset_name = st.text_input("Dataset name ì…ë ¥(í•„ìˆ˜)")
    session_id = st.text_input("ì„¸ì…˜ ID(ì„ íƒì‚¬í•­)")

    dataset_btn = st.button("ë°ì´í„°ì…‹ ë¡œë“œ")
    instructions = st.text_area("ì§€ì‹œì‚¬í•­", value="í•œê¸€ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”")

    if dataset_btn:
            # LangSmith ì„¤ì •
            st.session_state.examples = client.list_datasets(dataset_name=dataset_name)

            datasets = client.list_datasets(dataset_name=dataset_name)
            session_id = None if session_id.strip() == "" else session_id

            try:
                dataset = next(iter(datasets))
                qa_pairs = create_qa_pairs(dataset.id, session_id=session_id)
                st.session_state.examples = qa_pairs

                st.markdown(
                    f"`{dataset.name}`: `{dataset.id}`, ì´ {len(qa_pairs)} ê°œ QA ì„¸íŠ¸ ë¡œë“œ ì™„ë£Œ!"
                )

            except StopIteration:
                st.write("ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤.")


# ìœ ì €ì˜ ì…ë ¥ì„ ë°›ì•„ì„œ ëŒ€í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.
if user_input := st.chat_input():
        st.session_state.messages.append(ChatMessage(role="user", content=user_input))
        st.chat_message("user").write(user_input)
        with st.chat_message("assistant"):
            stream_handler = StreamHandler(st.empty())
            llm = ChatOpenAI(
                streaming=True,
                callbacks=[stream_handler],
            )
            if "examples" not in st.session_state:
                st.session_state.examples = []

            example_prompt = PromptTemplate(
                input_variables=["instruction", "question", "answer"],
                template="Question: {question}\nResponse: {answer}",
            )

            qa_pairs = st.session_state.examples
            prompt = FewShotPromptTemplate(
                examples=qa_pairs,
                example_prompt=example_prompt,
                suffix="----\nQuestion: {question}\nResponse: ",
                input_variables=["question"],
            )

            final_prompt = PromptTemplate.from_template(
                """Please refer to the INSTRUCTIONS and FEW SHOT examples to answer the question.
    #INSTUCTIONS:
    {instruction}

    #FEW EXAMPLES:
    {few_shot}                             
    """
            )

            chain = final_prompt | llm

            response = chain.invoke(
                {
                    "few_shot": prompt.format(question=user_input),
                    "instruction": instructions,
                }
            )

            st.session_state.messages.append(
                ChatMessage(role="assistant", content=response.content)
            )
